{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHAN_458Z-QT"
      },
      "source": [
        "![Workshop Banner](https://github.com/CLDiego/SPE_GeoHackathon_2025/blob/main/assets/S2_M1.png?raw=1)\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/CLDiego/SPE_GeoHackathon_2025/blob/main/S2_M1_RAG.ipynb)\n",
        "\n",
        "***\n",
        "# Session 02 // Module 01: Retrieval-Augmented Generation (RAG) for Petroleum Geoscience\n",
        "\n",
        "This module builds a practical RAG system over a curated geoscience dataset. You’ll ingest domain data, chunk and embed it, store it in a vector database, retrieve relevant context, and generate concise, cited answers with a local LLM.\n",
        "\n",
        "## Learning Objectives\n",
        "- Understand RAG components: splitter, embeddings, vector store, retriever, prompt, generator.\n",
        "- Build a local Chroma vector database from a Hugging Face dataset.\n",
        "- Compose LangChain Expression Language (LCEL) chains with chat history.\n",
        "- Diagnose retrieval quality and adjust k/chunking.\n",
        "- Ship an interactive Gradio app with citations.\n",
        "\n",
        "## What you’ll build\n",
        "- A reproducible RAG pipeline over geoscience content.\n",
        "- Conversational RAG with memory (RunnableWithMessageHistory).\n",
        "- A Gradio UI to ask domain questions and see cited sources."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7OwNlylHLsEg",
        "outputId": "8e9c1fdf-8769-4c1f-8e0c-494dc5de5bcd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.7/2.5 MB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/64.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/50.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.1/60.1 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.9/19.9 MB\u001b[0m \u001b[31m117.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m130.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.4/128.4 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m517.7/517.7 kB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m122.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m456.8/456.8 kB\u001b[0m \u001b[31m42.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m448.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Environment setup\n",
        "!pip -q install langchain langchain-core langchain-community langchain-huggingface\n",
        "# !pip -q install requests bitsandbytes transformers datasets accelerate\n",
        "!pip -q install requests==2.32.4 bitsandbytes transformers datasets accelerate\n",
        "!pip -q install langchain-chroma python-dotenv huggingface_hub\n",
        "!pip install -q --upgrade opentelemetry-api opentelemetry-sdk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pLIL5x5sMm2j"
      },
      "outputs": [],
      "source": [
        "# Hugging Face API token\n",
        "# Retrieving the token is required to get access to HF hub\n",
        "from google.colab import userdata\n",
        "HF_TOKEN = userdata.get('HF_TOKEN')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YoYx2tzzZ-QV"
      },
      "source": [
        "# 1. What is RAG?\n",
        "\n",
        "Retrieval-Augmented Generation (RAG) is a technique that enhances the capabilities of a Large Language Model (LLM) by providing it with external, up-to-date, and domain-specific information. Instead of relying solely on the knowledge baked into its weights during training, the LLM can access a knowledge base to ground its answers in facts.\n",
        "\n",
        "The core RAG workflow consists of two main phases:\n",
        "1.  **Indexing (Offline)**: We process our knowledge base (e.g., technical manuals, reports) by splitting documents into smaller chunks, converting them into numerical vectors (embeddings), and storing them in a specialized vector database.\n",
        "2.  **Retrieval & Generation (Online)**: When a user asks a question, we first retrieve the most relevant chunks from our database. Then, we feed both the question and the retrieved context to the LLM, instructing it to generate an answer based on the provided information.\n",
        "\n",
        "Why this helps:\n",
        "- **Reduces Hallucinations**: By anchoring the LLM's response to your specific data, it's less likely to invent facts.\n",
        "- **Enables Domain-Specific Knowledge**: You can build a chatbot for your proprietary documents without the massive cost of fine-tuning a model.\n",
        "- **Keeps Knowledge Fresh**: Updating the knowledge base is as simple as re-indexing your documents, which is much faster and cheaper than retraining an entire LLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Ji9_USwMGWq"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# Models and paths\n",
        "MODEL_EMBED = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "LLM_NAME = \"microsoft/Phi-3-mini-4k-instruct\"  # local HF model\n",
        "HF_DATASET = \"GainEnergy/ogdataset\"\n",
        "\n",
        "WORKDIR = Path.cwd()\n",
        "DATA_DIR = WORKDIR / \"raw_data\"\n",
        "DB_DIR = WORKDIR / \"local_data\" / \"geo_vector_db\"\n",
        "DB_DIR.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "CHUNK_SIZE = 1000\n",
        "CHUNK_OVERLAP = 200\n",
        "TOP_K = 8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5R35LIMZ-QV"
      },
      "source": [
        "# 2. Ingest and Structure the Corpus\n",
        "\n",
        "The first step in any RAG pipeline is to prepare the knowledge base. Here, we'll download a curated geoscience dataset from the Hugging Face Hub. Each record in the dataset contains text content and associated metadata (like title and topic).\n",
        "\n",
        "We will then transform these records into LangChain `Document` objects. This is a standard format that LangChain uses to represent pieces of text, making it easy to integrate with other components like text splitters and vector stores."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYuUbyBmZ-QV"
      },
      "source": [
        "> <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/code.svg\" width=\"20\"/> Key Parameters (`snapshot_download`)\n",
        "> - `repo_id`: The identifier of the Hugging Face repository (e.g., `\"GainEnergy/ogdataset\"`).\n",
        "> - `repo_type`: Specifies whether the repository is a `\"dataset\"` or `\"model\"`.\n",
        "> - `local_dir`: The local path where the repository files will be downloaded.\n",
        "> - `local_dir_use_symlinks`: Set to `False` to download the actual files instead of creating symlinks, which is more robust in environments like Google Colab.\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "85mIM44ENWm2"
      },
      "outputs": [],
      "source": [
        "# %%\n",
        "from huggingface_hub import snapshot_download, login\n",
        "\n",
        "if HF_TOKEN:\n",
        "    login(token=HF_TOKEN)\n",
        "\n",
        "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "repo_local_dir = DATA_DIR  # place under raw_data/\n",
        "snapshot_download(\n",
        "    repo_id=HF_DATASET,\n",
        "    repo_type=\"dataset\",\n",
        "    local_dir=str(repo_local_dir),\n",
        "    local_dir_use_symlinks=False,\n",
        ")\n",
        "\n",
        "# Find training_data.json (search recursively to be robust)\n",
        "json_candidates = list(repo_local_dir.rglob(\"training_data.json\"))\n",
        "if not json_candidates:\n",
        "    raise FileNotFoundError(\"Could not find training_data.json in raw_data/ after snapshot_download.\")\n",
        "json_path = json_candidates[0]\n",
        "print(f\"Using dataset file: {json_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQ1L2JYxZ-QW"
      },
      "source": [
        "### 2.1. Load and Transform Data\n",
        "\n",
        "Now that we have the data file, we'll load the JSON content and transform each record into a LangChain `Document`. A `Document` is a simple object that holds the text (`page_content`) and any associated `metadata` (like the source title or topic). This standardized format is crucial for compatibility with the rest of the LangChain ecosystem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xjG5MApTdBCm"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from typing import List, Dict, Any\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "with open(json_path, \"r\") as f:\n",
        "    data: List[Dict[str, Any]] = json.load(f)\n",
        "\n",
        "print(f\"Loaded {len(data)} records from {json_path.name}\")\n",
        "\n",
        "docs: List[Document] = []\n",
        "for rec in data:\n",
        "    text = (rec.get(\"content\") or \"\").strip()\n",
        "    if not text:\n",
        "        continue\n",
        "    meta = {\n",
        "        \"id\": rec.get(\"id\"),\n",
        "        \"topic\": rec.get(\"topic\"),\n",
        "        \"title\": rec.get(\"title\"),\n",
        "    }\n",
        "    docs.append(Document(page_content=text, metadata=meta))\n",
        "\n",
        "print(f\"Built {len(docs)} LangChain Documents. Example metadata:\", docs[0].metadata if docs else \"N/A\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m5AHMj7PftKA"
      },
      "outputs": [],
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_chroma import Chroma\n",
        "\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\n",
        "chunks = splitter.split_documents(docs)\n",
        "print(f\"Split {len(docs)} documents into {len(chunks)} chunks.\")\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(model_name=MODEL_EMBED)\n",
        "\n",
        "# Recreate vectorstore fresh (persisted locally)\n",
        "if DB_DIR.exists():\n",
        "    import shutil\n",
        "    shutil.rmtree(DB_DIR)\n",
        "\n",
        "vectorstore = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=str(DB_DIR))\n",
        "print(f\"Vectorstore created and persisted at: {DB_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "muVHi1MEZ-QX"
      },
      "source": [
        "# 3. Building the RAG Pipeline\n",
        "\n",
        "Now we assemble the core components of our RAG pipeline: the text splitter, the embedding model, and the vector store."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxPql7C9Z-QX"
      },
      "source": [
        "### 3.1. Text Splitting and Embedding\n",
        "\n",
        "LLMs have a limited context window, so we can't feed them entire documents at once. We must split our documents into smaller, manageable chunks. The `RecursiveCharacterTextSplitter` is a smart way to do this, as it tries to keep related pieces of text together by splitting on paragraphs, sentences, and then characters as a last resort.\n",
        "\n",
        "> <img src=\"https://github.com/CLDiego/uom_fse_dl_workshop/raw/main/figs/icons/write.svg\" width=\"20\"/> Key Parameters (`RecursiveCharacterTextSplitter`)\n",
        "> - `chunk_size`: The maximum number of characters in each chunk. A good starting point is 500-1000.\n",
        "> - `chunk_overlap`: The number of characters to overlap between adjacent chunks. This helps maintain context across chunk boundaries. A common value is 10-20% of the chunk size.\n",
        "\n",
        "Once split, each chunk is converted into a numerical vector using an **embedding model**. We use `sentence-transformers/all-MiniLM-L6-v2`, a fast and effective model for this task. These vectors capture the semantic meaning of the text.\n",
        "\n",
        "### 3.2. Vector Store\n",
        "\n",
        "The final step is to store these embeddings in a **vector store** for efficient retrieval. We use `Chroma`, a popular open-source vector database that can run locally. By persisting the database to disk, we can reuse it in future runs without re-indexing the documents every time.\n",
        "\n",
        "***\n",
        "\n",
        "The next cell initializes the text splitter, creates embeddings for the document chunks, and builds a persistent Chroma vector store. We include a step to remove any existing database to ensure we start fresh."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NoeVB9-VZ-QX"
      },
      "source": [
        "### 3.3. Generation Pipeline\n",
        "\n",
        "With our knowledge base indexed, we now need the \"G\" in RAG: the **Generator**. This is the LLM that will synthesize an answer based on the user's question and the retrieved context. We'll load `microsoft/Phi-3-mini-4k-instruct`, a powerful yet relatively small model suitable for running in Colab.\n",
        "\n",
        "To optimize performance, we'll use 4-bit quantization via `bitsandbytes`, which significantly reduces the model's memory footprint with minimal impact on quality. The `build_generation_pipeline` function below handles device detection (CUDA for NVIDIA GPUs, MPS for Apple Silicon, CPU as a fallback) and configures the model and tokenizer accordingly.\n",
        "\n",
        "Finally, we wrap the Hugging Face `pipeline` in a `LangChainHuggingFace` object to make it compatible with the LangChain Expression Language (LCEL)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l0MnQtrWsQEY"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "def build_generation_pipeline(model_id: str):\n",
        "    use_cuda = torch.cuda.is_available()\n",
        "    use_mps = getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available()\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "    quantization_config = None\n",
        "    model_kwargs = {}\n",
        "    if use_cuda:\n",
        "        # Optional 4-bit if bitsandbytes available\n",
        "        try:\n",
        "            from transformers import BitsAndBytesConfig\n",
        "            quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
        "            model_kwargs[\"quantization_config\"] = quantization_config\n",
        "            model_kwargs[\"device_map\"] = \"auto\"\n",
        "            print(\"Using CUDA with 4-bit quantization.\")\n",
        "        except Exception:\n",
        "            model_kwargs[\"device_map\"] = \"auto\"\n",
        "            model_kwargs[\"torch_dtype\"] = torch.float16\n",
        "            print(\"Using CUDA without bitsandbytes 4-bit (fallback).\")\n",
        "    elif use_mps:\n",
        "        model_kwargs[\"torch_dtype\"] = torch.float16\n",
        "        model_kwargs[\"device_map\"] = \"auto\"\n",
        "        print(\"Using Apple MPS (float16).\")\n",
        "    else:\n",
        "        model_kwargs[\"torch_dtype\"] = torch.float32\n",
        "        model_kwargs[\"device_map\"] = \"auto\"\n",
        "        print(\"Using CPU (this will be slow).\")\n",
        "\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_id, **model_kwargs)\n",
        "\n",
        "    # Ensure pad token is set\n",
        "    if tokenizer.pad_token_id is None:\n",
        "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "    gen_pipe = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        max_new_tokens=256,\n",
        "        temperature=0.2,\n",
        "        do_sample=False,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "        return_full_text=False,\n",
        "    )\n",
        "    return gen_pipe\n",
        "\n",
        "gen_pipe = build_generation_pipeline(LLM_NAME)\n",
        "\n",
        "# Wrap as LangChain LLM\n",
        "from langchain_huggingface import HuggingFacePipeline\n",
        "llm = HuggingFacePipeline(pipeline=gen_pipe)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGcnutF0Z-QX"
      },
      "source": [
        "### 3.4. Composing the RAG Chain with Memory\n",
        "\n",
        "Now, we'll tie everything together using the **LangChain Expression Language (LCEL)**. This declarative style makes the flow of data transparent and easy to modify. Here are the key steps:\n",
        "\n",
        "1.  **Retriever**: We create a retriever from our `vectorstore`. This component takes a user query, embeds it, and retrieves the most relevant document chunks from the vector database. The `search_kwargs={\"k\": TOP_K}` parameter controls how many chunks to retrieve.\n",
        "\n",
        "2.  **Prompt Template**: We design a `ChatPromptTemplate` to structure the input for the LLM. It includes:\n",
        "    - A `system` message to define the assistant's persona and instructions.\n",
        "    - A `MessagesPlaceholder` to inject the conversation history.\n",
        "    - A `human` message that combines the retrieved `{context}` and the user's `{input}`.\n",
        "\n",
        "3.  **LCEL Chain**: We compose our chain using the `|` (pipe) operator. This is a more modern and explicit way to define the RAG flow compared to using helper functions like `create_retrieval_chain`.\n",
        "    - We use `RunnablePassthrough.assign` to pass the retrieved documents into the `context` key for the prompt.\n",
        "    - The `RunnableParallel` dictionary (`{\"context\": retriever, \"input\": RunnablePassthrough()}`) is a key step. It invokes the retriever and passes the original user input through simultaneously.\n",
        "\n",
        "4.  **Memory**: To enable multi-turn conversations, we wrap our RAG chain with `RunnableWithMessageHistory`. This powerful runnable automatically manages chat history. It uses a `get_history` function to load and save messages for a given `session_id`, allowing the bot to remember previous turns in the conversation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-RIaEJL7u57d"
      },
      "outputs": [],
      "source": [
        "# %%\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain.chains import create_retrieval_chain\n",
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "from langchain_community.chat_message_histories import ChatMessageHistory\n",
        "\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": TOP_K})\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a helpful assistant for geology and petroleum engineering. \"\n",
        "                   \"Use the provided context to answer the question. If unsure, say you don't know. \"\n",
        "                   \"Cite titles when possible.\"),\n",
        "        MessagesPlaceholder(\"chat_history\"),\n",
        "        (\"human\", \"Context:\\n{context}\\n\\nQuestion: {input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Stuff documents into the prompt\n",
        "document_chain = create_stuff_documents_chain(llm, prompt)\n",
        "rag_chain = create_retrieval_chain(retriever, document_chain)  # returns {\"answer\", \"context\"}\n",
        "\n",
        "# Attach message history\n",
        "_store: dict[str, ChatMessageHistory] = {}\n",
        "\n",
        "def get_history(session_id: str) -> ChatMessageHistory:\n",
        "    if session_id not in _store:\n",
        "        _store[session_id] = ChatMessageHistory()\n",
        "    return _store[session_id]\n",
        "\n",
        "conv_rag = RunnableWithMessageHistory(\n",
        "    rag_chain,\n",
        "    get_history,\n",
        "    input_messages_key=\"input\",\n",
        "    history_messages_key=\"chat_history\",\n",
        "    output_messages_key=\"answer\",\n",
        ")\n",
        "\n",
        "print(\"RAG chain with chat history initialized.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSAC4U11Z-QY"
      },
      "source": [
        "# 4. Querying and Diagnostics\n",
        "\n",
        "With the conversational RAG chain built, we can now ask it questions. We'll also create some diagnostic functions to inspect the retrieval process, which is crucial for tuning and troubleshooting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gGcAQbZM0t0w"
      },
      "outputs": [],
      "source": [
        "from typing import Tuple, List\n",
        "\n",
        "def ask(question: str, session_id: str = \"default\") -> Tuple[str, List[Document]]:\n",
        "    result = conv_rag.invoke(\n",
        "        {\"input\": question},\n",
        "        config={\"configurable\": {\"session_id\": session_id}},\n",
        "    )\n",
        "    answer = result.get(\"answer\", \"\")\n",
        "    source_docs = result.get(\"context\", [])  # list[Document]\n",
        "    print(\"Answer:\\n\", answer)\n",
        "    print(\"\\nCitations:\")\n",
        "    for i, d in enumerate(source_docs, 1):\n",
        "        md = d.metadata or {}\n",
        "        title = md.get(\"title\") or md.get(\"topic\") or \"Untitled\"\n",
        "        print(f\"[{i}] {title}\")\n",
        "    return answer, source_docs\n",
        "\n",
        "def rag_query(query: str, k: int = 3) -> str:\n",
        "    # Build a temporary retrieval chain to allow custom k\n",
        "    tmp_retriever = vectorstore.as_retriever(search_kwargs={\"k\": k})\n",
        "    tmp_chain = create_retrieval_chain(tmp_retriever, document_chain)\n",
        "    result = tmp_chain.invoke({\"input\": query})\n",
        "    if isinstance(result, dict):\n",
        "        return result.get(\"answer\", \"\")\n",
        "    return str(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "preview-retrieval-cell"
      },
      "outputs": [],
      "source": [
        "# 3. Retrieval diagnostics: see what you're feeding the LLM\n",
        "from textwrap import shorten\n",
        "\n",
        "def preview_retrieval(query: str, k: int = 5):\n",
        "    print(f\"Query: {query}\\n---\")\n",
        "    docs_scores = []\n",
        "    try:\n",
        "        # Chroma supports similarity_search_with_score\n",
        "        docs_scores = vectorstore.similarity_search_with_score(query, k=k)\n",
        "    except Exception:\n",
        "        # Fallback (no scores)\n",
        "        docs = vectorstore.similarity_search(query, k=k)\n",
        "        docs_scores = [(d, None) for d in docs]\n",
        "\n",
        "    for i, (doc, score) in enumerate(docs_scores, 1):\n",
        "        md = doc.metadata or {}\n",
        "        title = md.get(\"title\") or md.get(\"topic\") or \"Untitled\"\n",
        "        snippet = shorten(doc.page_content, width=180, placeholder=\" ...\")\n",
        "        score_str = f\" | score={score:.4f}\" if score is not None else \"\"\n",
        "        print(f\"[{i}] {title}{score_str}\\n    {snippet}\\n\")\n",
        "\n",
        "# Example diagnostics\n",
        "preview_retrieval(\"What factors control porosity and permeability in clastic reservoirs?\", k=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0MBJTMOZ-QY"
      },
      "source": [
        "The `preview_retrieval` function is a vital diagnostic tool. It lets you see exactly which document chunks are being retrieved for a given query and what their similarity scores are. This helps you answer critical questions:\n",
        "- Are the retrieved chunks relevant to the query?\n",
        "- Is `k` (the number of chunks) too high or too low?\n",
        "- Is the `chunk_size` appropriate? If snippets are too short or too long, you may need to adjust your splitting strategy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SyIf--MeZ-QZ"
      },
      "outputs": [],
      "source": [
        "# %%\n",
        "test_questions = [\n",
        "    \"What factors control porosity and permeability in clastic reservoirs?\",\n",
        "    \"What standards apply to BOP?\",\n",
        "]\n",
        "for q in test_questions:\n",
        "    print(\"\\n=== Q:\", q)\n",
        "    ask(q)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fpGH5Vp9Z-QZ"
      },
      "source": [
        "### 4.2. Testing the Full Chain\n",
        "\n",
        "Let's run our test questions through the complete `conv_rag` chain to see the final, context-aware answers and their cited sources."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gradio-app-cell"
      },
      "outputs": [],
      "source": [
        "# %%\n",
        "# 5. Gradio app: Conversational RAG with citations\n",
        "import uuid\n",
        "import gradio as gr\n",
        "from langchain.chains import create_retrieval_chain\n",
        "\n",
        "def build_conv(k: int = TOP_K, search_type: str = \"similarity\"):\n",
        "    # retr = vectorstore.as_retriever(search_kwargs={\"k\": k, \"search_type\": search_type}) # Chroma does not support search_type\n",
        "    retr = vectorstore.as_retriever(search_kwargs={\"k\": k})\n",
        "    chain = create_retrieval_chain(retr, document_chain)\n",
        "    conv = RunnableWithMessageHistory(\n",
        "        chain,\n",
        "        get_history,\n",
        "        input_messages_key=\"input\",\n",
        "        history_messages_key=\"chat_history\",\n",
        "        output_messages_key=\"answer\",\n",
        "    )\n",
        "    return conv\n",
        "\n",
        "def format_citations(docs: List[Document]) -> str:\n",
        "    lines = []\n",
        "    seen = set()\n",
        "    for i, d in enumerate(docs, 1):\n",
        "        md = d.metadata or {}\n",
        "        title = md.get(\"title\") or md.get(\"topic\") or \"Untitled\"\n",
        "        if title in seen:\n",
        "            continue\n",
        "        seen.add(title)\n",
        "        lines.append(f\"- [{i}] {title}\")\n",
        "    return \"\\n\".join(lines) if lines else \"_No citations_\"\n",
        "\n",
        "def respond(message, chat_history, sid, k, search_type):\n",
        "    if not sid:\n",
        "        sid = str(uuid.uuid4())\n",
        "    conv = build_conv(k=int(k), search_type=search_type)\n",
        "    result = conv.invoke({\"input\": message}, config={\"configurable\": {\"session_id\": sid}})\n",
        "    answer = result.get(\"answer\", \"\")\n",
        "    ctx = result.get(\"context\", [])\n",
        "    citations_md = format_citations(ctx)\n",
        "    chat_history = chat_history + [[message, answer]]\n",
        "    return chat_history, sid, citations_md\n",
        "\n",
        "with gr.Blocks(title=\"Geo RAG Assistant\") as demo:\n",
        "    gr.Markdown(\"## Geo RAG Assistant\\nAsk petroleum geoscience questions grounded on the local corpus.\")\n",
        "    with gr.Row():\n",
        "        chatbot = gr.Chatbot(height=350)\n",
        "        with gr.Column():\n",
        "            citations = gr.Markdown(value=\"_Citations will appear here_\")\n",
        "            k_slider = gr.Slider(1, 12, value=TOP_K, step=1, label=\"Top-k\")\n",
        "            search_type = gr.Radio(choices=[\"similarity\", \"mmr\"], value=\"similarity\", label=\"Search type\")\n",
        "            sid = gr.Textbox(value=\"\", label=\"Session ID (auto if blank)\")\n",
        "\n",
        "    msg = gr.Textbox(placeholder=\"Type your question about porosity, BOP standards, etc.\")\n",
        "    send = gr.Button(\"Ask\")\n",
        "    clear = gr.Button(\"Clear chat\")\n",
        "\n",
        "    def on_clear():\n",
        "        return [], \"\", \"_Citations will appear here_\"\n",
        "\n",
        "    send.click(respond, [msg, chatbot, sid, k_slider, search_type], [chatbot, sid, citations])\n",
        "    clear.click(on_clear, outputs=[chatbot, sid, citations])\n",
        "\n",
        "try:\n",
        "    demo.launch(share=False)\n",
        "except Exception as e:\n",
        "    print(\"Gradio failed to launch in this environment:\", e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gG7LIcaZ-QZ"
      },
      "source": [
        "## 5. Troubleshooting and tuning\n",
        "\n",
        "- Empty or weak answers\n",
        "  - Lower k (reduce noise) or increase k (capture missing facts).\n",
        "  - Reduce CHUNK_SIZE or increase CHUNK_OVERLAP for finer granularity.\n",
        "  - Verify dataset loaded: print head of docs and metadata.\n",
        "\n",
        "- Slow generation\n",
        "  - Lower max_new_tokens in the HF pipeline.\n",
        "  - Use smaller models; ensure MPS/CUDA is active.\n",
        "  - Reduce k to fetch fewer chunks.\n",
        "\n",
        "- macOS (MPS) quantization\n",
        "  - BitsAndBytes 4-bit is not supported; use float16 on MPS or CPU float32.\n",
        "  - Switch to small models (≤2B) if resources are limited.\n",
        "\n",
        "- Re-running faster\n",
        "  - Skip re-embedding by reusing the persisted Chroma DB (don’t delete DB_DIR).\n",
        "  - Cache dataset snapshot on disk.\n",
        "\n",
        "- Better citations\n",
        "  - Encourage titles in the system prompt, or render citations below the answer (as we do in Gradio)."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}